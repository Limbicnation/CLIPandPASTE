{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/CLIPandPASTE/blob/main/CLIP_and_PASTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ysKe2XpxRFd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install kornia\n",
        "!pip install ftfy\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps\n",
        "!pip install boto3 > /dev/null 2>&1\n",
        "!gdown --id 1TS5K0BGk5ruCF-bc6yeMSAEb5z8Oi_st\n",
        "!gdown --id 1-2ForMsp58l6DVAeUqEvW0N24-YITf5o\n",
        "!wget https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.10.0/index.html\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "!pip install -e .\n",
        "!mkdir checkpoints\n",
        "!wget -c https://download.openmmlab.com/mmdetection/v2.0/groie/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco_20200604_211715-42eb79e1.pth \\\n",
        "      -O checkpoints/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco_20200604_211715-42eb79e1.pth\n",
        "%cd /content\n",
        "\n",
        "!rm -r open_images\n",
        "!rm -r wiki_images\n",
        "!mkdir open_images\n",
        "!mkdir wiki_images\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode\n",
        "import kornia\n",
        "import numpy as np\n",
        "import pickle\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import re\n",
        "import clip\n",
        "import torch\n",
        "import warnings\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import random\n",
        "import nltk\n",
        "import pke\n",
        "import matplotlib.pylab as plb\n",
        "import torchvision.transforms as T\n",
        "# from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode\n",
        "import torchvision\n",
        "import sys\n",
        "sys.path.append(\"/content/mmdetection\")\n",
        "import mmdet\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "import IPython\n",
        "from shapely.geometry import Polygon\n",
        "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
        "\n",
        "config = '/content/mmdetection/configs/groie/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco.py'\n",
        "checkpoint = '/content/mmdetection/checkpoints/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco_20200604_211715-42eb79e1.pth'\n",
        "groie_model = init_detector(config, checkpoint, device='cuda:0')\n",
        "\n",
        "text_features16 = np.load(\"ai-memer_embeddings16.npy\")\n",
        "print(text_features16.shape)\n",
        "annotations = pickle.load(open(\"ai-memer_annotations.pkl\", \"rb\"))\n",
        "print(annotations[520000])\n",
        "\n",
        "device = torch.device('cuda')\n",
        "clip_model, clip_preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "\n",
        "def get_text_features(prompt):\n",
        "  text_input = clip.tokenize(prompt).to(device)\n",
        "  with torch.no_grad():\n",
        "    text_features = clip_model.encode_text(text_input)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  return(text_features)\n",
        "\n",
        "def get_top_N_semantic_similarity(similarity_list, N):\n",
        "  results = zip(range(len(similarity_list)), similarity_list)\n",
        "  results = sorted(results, key=lambda x: x[1],reverse = True)\n",
        "  scores = []\n",
        "  indices = []\n",
        "  for index,score in results[:N]:\n",
        "    scores.append(score)\n",
        "    indices.append(index)\n",
        "  return scores, indices\n",
        "\n",
        "extractor = pke.unsupervised.TopicRank()\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "print(stop_words)\n",
        "\n",
        "def download_file(url, path):\n",
        "  filename = url.split(\"/\")[-1]\n",
        "  file_path = os.path.join(path, filename)\n",
        "  headers = {'User-Agent': 'CLIPandPASTE/1.0 (https://robgon.medium.com/; robgon.art@gmail.com)'}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  file = open(file_path, \"wb\")\n",
        "  file.write(response.content)\n",
        "  file.close()\n",
        "\n",
        "coco_names = [\n",
        "        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n",
        "        'truck', 'boat', 'traffic_light', 'fire_hydrant', 'stop_sign',\n",
        "        'parking_meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "        'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
        "        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
        "        'sports_ball', 'kite', 'baseball_bat', 'baseball_glove', 'skateboard',\n",
        "        'surfboard', 'tennis_racket', 'bottle', 'wine_glass', 'cup', 'fork',\n",
        "        'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
        "        'broccoli', 'carrot', 'hot_dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "        'couch', 'potted_plant', 'bed', 'dining_table', 'toilet', 'tv',\n",
        "        'laptop', 'mouse', 'remote', 'keyboard', 'cell_phone', 'microwave',\n",
        "        'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "        'scissors', 'teddy_bear', 'hair_drier', 'toothbrush']\n",
        "  \n",
        "colors = [[random.randint(0, 255) for _ in range(3)] for _ in coco_names]\n",
        "img_size = 224\n",
        "\n",
        "# from https://stackoverflow.com/questions/61616810/how-to-do-cubic-spline-interpolation-and-integration-in-pytorch\n",
        "\n",
        "def h_poly_helper(tt):\n",
        "  A = torch.tensor([\n",
        "      [1,  0, -3,  2],\n",
        "      [0,  1, -2,  1],\n",
        "      [0,  0,  3, -2],\n",
        "      [0,  0, -1,  1]\n",
        "      ], dtype=tt[-1].dtype)\n",
        "  return [\n",
        "    sum( A[i, j]*tt[j] for j in range(4) )\n",
        "    for i in range(4) ]\n",
        "\n",
        "def h_poly(t):\n",
        "  tt = [ None for _ in range(4) ]\n",
        "  tt[0] = 1\n",
        "  for i in range(1, 4):\n",
        "    tt[i] = tt[i-1]*t\n",
        "  return h_poly_helper(tt)\n",
        "\n",
        "def interp(x, y, xs):\n",
        "  m = (y[1:] - y[:-1])/(x[1:] - x[:-1])\n",
        "  m = torch.cat([m[[0]], (m[1:] + m[:-1])/2, m[[-1]]])\n",
        "  I = plb.searchsorted(x[1:], xs)\n",
        "  dx = (x[I+1]-x[I])\n",
        "  hh = h_poly((xs-x[I])/dx)\n",
        "  return hh[0]*y[I] + hh[1]*m[I]*dx + hh[2]*y[I+1] + hh[3]*m[I+1]*dx\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    poly1 = Polygon([[box1[0], box1[1]], [box1[2], box1[1]], [box1[2], box1[3]], [box1[0], box1[3]]])\n",
        "    poly2 = Polygon([[box2[0], box2[1]], [box2[2], box2[1]], [box2[2], box2[3]], [box2[0], box2[3]]])\n",
        "    iou = poly1.intersection(poly2).area / poly1.union(poly2).area\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lhlBo_vmJHk"
      },
      "outputs": [],
      "source": [
        "prompt = \"penguins skiing down a snowy mountain\" #@param {type:\"string\"}\n",
        "prompt = prompt.lower()\n",
        "num_keywords = 10\n",
        "\n",
        "extractor.load_document(input=prompt, language='en')\n",
        "extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ', 'VERB'})\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "keyphrases = extractor.get_n_best(n=num_keywords, stemming=False)\n",
        "print(keyphrases)\n",
        "keywords = [prompt]\n",
        "for i, (candidate, score) in enumerate(keyphrases):   \n",
        "  print(\"rank {}: {} ({})\".format(i, candidate, score))\n",
        "  if candidate not in keywords:\n",
        "    keywords.append(candidate)\n",
        "\n",
        "words = prompt.split()\n",
        "for w in words:\n",
        "  if w not in keywords and w not in stop_words:\n",
        "    keywords.append(w)\n",
        "\n",
        "keywords = keywords[:10]\n",
        "print(keywords)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "all_images = []\n",
        "all_files = []\n",
        "total_images = 32\n",
        "num_openimages = 0\n",
        "num_wikiimages = 0\n",
        "\n",
        "num_images = int(round(float(total_images)/(len(keywords)+1)+0.5))\n",
        "\n",
        "for j in range(len(keywords)):\n",
        "  print(\"looking for\" , num_images*2, \"images of\", keywords[j])\n",
        "  feature_text = \"<|startoftext|> Image of \" + keywords[j] + \" <|endoftext|>\"\n",
        "  query_features = get_text_features(feature_text)\n",
        "  text_similarity = query_features.cpu().numpy() @ text_features16.T\n",
        "  text_similarity = text_similarity[0]\n",
        "  text_scores, text_indices = get_top_N_semantic_similarity(text_similarity, N=num_images)\n",
        "\n",
        "  # get the images from OpenImages\n",
        "  # print(\"Downloading images from OpenImages\")\n",
        "  f = open(\"images.txt\", \"w\")\n",
        "  for i in text_indices:\n",
        "    f.write(annotations[i][0] +\"\\n\")\n",
        "  f.close()\n",
        "  !python downloader.py images.txt --download_folder=open_images --num_processes=1 > /dev/null 2>&1\n",
        "  for i in range(0, num_images):\n",
        "    image_id = annotations[text_indices[i]][0]\n",
        "    parts = image_id.split(\"/\")\n",
        "    file_path = \"open_images/\" + parts[1] + \".jpg\"\n",
        "    if file_path not in all_files:\n",
        "      # print(file_path)\n",
        "      img = Image.open(file_path)\n",
        "      img = img.convert(mode=\"RGB\")\n",
        "      all_images.append(img)\n",
        "      all_files.append(file_path)\n",
        "      num_openimages += 1\n",
        "\n",
        "  # get the images Wikipedia\n",
        "  s = requests.Session()\n",
        "  url = \"https://commons.wikimedia.org/w/api.php\"\n",
        "  params = {\n",
        "      \"action\": \"query\",\n",
        "      \"generator\": \"images\",\n",
        "      \"prop\": \"imageinfo\",\n",
        "      \"gimlimit\": 500,\n",
        "      \"titles\": keywords[j],\n",
        "      \"iiprop\": \"url|dimensions\",\n",
        "      \"format\": \"json\"\n",
        "  }\n",
        "  r = s.get(url=url, params=params)\n",
        "  data = r.json()\n",
        "  image_files = []\n",
        "  if \"query\" not in data.keys():\n",
        "    continue\n",
        "  pages = data['query']['pages']\n",
        "  for k, v in pages.items():\n",
        "    for info in v['imageinfo']:\n",
        "      imurl = info[\"url\"]\n",
        "      h =  info[\"height\"]\n",
        "      w = info[\"width\"]\n",
        "      a = h * w\n",
        "      if a >= 512*512 and imurl not in image_files and imurl.lower().endswith(\"jpg\"):\n",
        "        image_files.append(imurl)\n",
        "  random.shuffle(image_files)\n",
        "  for im in image_files[:num_images]:\n",
        "    filename = im.split(\"/\")[-1]\n",
        "    download_file(im, \"wiki_images\")\n",
        "    file_path = \"wiki_images/\" + filename\n",
        "    if file_path not in all_files:\n",
        "      # print(file_path)\n",
        "      img = Image.open(file_path)\n",
        "      img = img.convert(mode=\"RGB\")\n",
        "      all_images.append(img)\n",
        "      all_files.append(file_path)\n",
        "      num_wikiimages += 1\n",
        "\n",
        "print(\"num openimages  \", num_openimages)\n",
        "print(\"num wiki images \", num_wikiimages)\n",
        "print(\"num total images\", num_openimages+num_wikiimages)\n",
        "\n",
        "input_resolution = 224\n",
        "image_features = torch.empty((0, 512))\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "images = [preprocess(im) for im in all_images]\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "  image_features = clip_model.encode_image(image_input).float().cpu()  \n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "feature_text = \"<|startoftext|> Image of \" + prompt + \" <|endoftext|>\"\n",
        "query_features = get_text_features(feature_text)\n",
        "\n",
        "image_similarity = query_features.cpu().numpy() @ image_features.numpy().T\n",
        "image_similarity = image_similarity[0]\n",
        "print(len(all_files))\n",
        "\n",
        "num_images = min(int(0.75*len(all_files)), 25)\n",
        "image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=num_images)\n",
        "columns = 5\n",
        "rows = num_images // columns + 1\n",
        "fig=plt.figure(figsize=(columns*5, rows*5))\n",
        "for i in range(1, columns*rows + 1):\n",
        "  file_name = all_files[image_indices[i-1]]\n",
        "  img = Image.open(file_name)\n",
        "  img = img.convert(mode=\"RGB\")\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.margins(y=10)\n",
        "  plt.imshow(img)\n",
        "  plt.text(0, -30, str(i) + \" \" + file_name, fontsize=10)\n",
        "  plt.axis(\"off\")\n",
        "  if i >= num_images:\n",
        "    break\n",
        "plt.show()\n",
        "\n",
        "image_parts = []\n",
        "parts_rgb = []\n",
        "parts_a = []\n",
        "part_sizes = {}\n",
        "part_count = 0\n",
        "\n",
        "preprocess_parts = Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "for i in range(num_images):\n",
        "  image_file = all_files[image_indices[i]]\n",
        "  print(i, file_name)\n",
        "\n",
        "  input_image = Image.open(image_file).convert(mode=\"RGB\")\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.axis(\"off\")\n",
        "  _ = plt.imshow(input_image)\n",
        "  plt.show()\n",
        "\n",
        "  result = inference_detector(groie_model, image_file)\n",
        "  bbox_result, segm_result = result\n",
        "\n",
        "  boxes = []\n",
        "  overlaps = []\n",
        "  scores = []\n",
        "  labels = []\n",
        "  mask_areas = []\n",
        "  result_image = np.array(input_image.copy())\n",
        "  count = 0\n",
        "\n",
        "  # print()\n",
        "  # print(\"objects\")\n",
        "\n",
        "  for label, boxscores in enumerate(bbox_result):\n",
        "    for boxscore in boxscores:\n",
        "      box = boxscore[:4]\n",
        "      score = boxscore[4]\n",
        "\n",
        "      overlapping = False\n",
        "      for b in boxes:\n",
        "        overlap = calculate_iou(box, b)\n",
        "        # print(\"overlap\", overlap)\n",
        "        if  overlap > 0.85:\n",
        "          overlapping = True\n",
        "          # print(\"skipping\")\n",
        "          break\n",
        "\n",
        "      overlaps.append(overlapping)\n",
        "\n",
        "      boxes.append(box)\n",
        "      scores.append(score)\n",
        "      labels.append(label)\n",
        "      # print(label, coco_names[label], box, score)\n",
        "\n",
        "      if overlapping:\n",
        "        continue\n",
        "\n",
        "      color = random.choice(colors)\n",
        "      # print(count+1, coco_names[label], round(100*score.item(), 2))\n",
        "      # draw box\n",
        "      tl = round(0.001 * max(result_image.shape[0:2])) + 1  # line thickness\n",
        "      c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "      cv2.rectangle(result_image, c1, c2, color, thickness=tl)\n",
        "      # draw text\n",
        "      display_txt = \"%s: %.1f%%\" % (coco_names[label], 100*score)\n",
        "      tf = max(tl - 1, 1)  # font thickness\n",
        "      t_size = cv2.getTextSize(display_txt, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "      c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "      cv2.rectangle(result_image, c1, c2, color, -1)  # filled\n",
        "      cv2.putText(result_image, display_txt, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "      count += 1\n",
        "\n",
        "  if count == 0:\n",
        "    print(\"no objects found\")\n",
        "    continue\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.axis(\"off\")\n",
        "  _ = plt.imshow(result_image)\n",
        "  plt.show()\n",
        "\n",
        "  count = 0\n",
        "  masks = []\n",
        "  mask_accum = None\n",
        "  for object_masks in segm_result:\n",
        "    for mask in object_masks:\n",
        "      mask_np = np.float32(mask)\n",
        "      masks.append(mask_np)\n",
        "      mask_area = mask_np.sum() / (input_image.width*input_image.height)\n",
        "      mask_areas.append(mask_area)\n",
        "      if mask_accum is None:\n",
        "        mask_accum = mask\n",
        "      else:\n",
        "        mask_accum = np.maximum(mask_accum, mask)\n",
        "      count += 1\n",
        "\n",
        "  images = []\n",
        "\n",
        "  img_wid = input_image.width\n",
        "  img_hgt = input_image.height\n",
        "\n",
        "  count = 0\n",
        "  for box, overlapping, score, mask_np in zip(boxes, overlaps, scores, masks):\n",
        "    # print(box)\n",
        "\n",
        "    mask_np = np.expand_dims(mask_np, axis=2)\n",
        "    num_mask_pixels = mask_np.sum()\n",
        "\n",
        "    if overlapping or num_mask_pixels < 1000:\n",
        "      continue\n",
        "\n",
        "    mask_np = np.repeat(mask_np, 3, axis=2)\n",
        "    image_np = np.array(input_image, dtype=np.float32)/255.0\n",
        "\n",
        "    if mask_np.shape != image_np.shape:\n",
        "      continue\n",
        "\n",
        "    masked_image_np = mask_np * image_np\n",
        "\n",
        "    box_lft = int(box[0].item())\n",
        "    box_top = int(box[1].item())\n",
        "    box_rgt = int(box[2].item())\n",
        "    box_bot = int(box[3].item())\n",
        "\n",
        "    cutout_image_np = masked_image_np[box_top:box_bot, box_lft:box_rgt]\n",
        "    cutout_mask_np = mask_np[box_top:box_bot, box_lft:box_rgt]\n",
        "\n",
        "    box_wid = box_rgt - box_lft\n",
        "    box_hgt = box_bot - box_top\n",
        "\n",
        "    if box_wid > box_hgt: # handle landscape images\n",
        "      # print(\"landscape\")\n",
        "      pad = (box_wid - box_hgt) // 2\n",
        "      padded_image_np = np.zeros((box_wid, box_wid, 3), dtype=cutout_image_np.dtype)\n",
        "      padded_image_np[pad:pad+box_hgt, :] = cutout_image_np\n",
        "    \n",
        "    else: # handle portrait images\n",
        "      # print(\"portrait\")\n",
        "      pad = (box_hgt - box_wid) // 2\n",
        "      padded_image_np = np.zeros((box_hgt, box_hgt, 3), dtype=cutout_image_np.dtype)\n",
        "      padded_image_np[:, pad:pad+box_wid] = cutout_image_np     \n",
        "\n",
        "    image_PIL = Image.fromarray(np.uint8(padded_image_np*255))\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.axis(\"off\")\n",
        "    _ = plt.imshow(image_PIL)\n",
        "    plt.show()\n",
        "\n",
        "    image_parts.append(preprocess(image_PIL))\n",
        "\n",
        "    part_PIL = Image.fromarray(np.uint8(cutout_image_np*255))\n",
        "    w, h = part_PIL.size\n",
        "    if w > h and w > 512:\n",
        "      part_pil = part_PIL.resize((512, int(h*512/w)), Image.BICUBIC)\n",
        "    elif h > 512:\n",
        "      part_pil = part_PIL.resize((int(w*512/h), 512), Image.BICUBIC)\n",
        "\n",
        "    parts_rgb.append(preprocess_parts(part_PIL))\n",
        "    mask_PIL = Image.fromarray(np.uint8(cutout_mask_np*255))\n",
        "    parts_a.append(preprocess_parts(mask_PIL))\n",
        "    part_sizes[part_count] = num_mask_pixels\n",
        "    count += 1\n",
        "    part_count += 1\n",
        "\n",
        "to_pil = T.ToPILImage()\n",
        "\n",
        "num_parts = min(len(image_parts)//2,100)\n",
        "\n",
        "part_input = torch.tensor(np.stack(image_parts)).cuda()\n",
        "with torch.no_grad():\n",
        "  part_features = clip_model.encode_image(part_input).float().cpu()  \n",
        "part_features /= part_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "feature_text = \"<|startoftext|> Image of \" + prompt + \" <|endoftext|>\"\n",
        "query_features = get_text_features(feature_text)\n",
        "\n",
        "part_similarity = query_features.cpu().numpy() @ part_features.numpy().T\n",
        "part_similarity = part_similarity[0]\n",
        "\n",
        "part_scores, part_indices = get_top_N_semantic_similarity(part_similarity, N=num_parts)\n",
        "columns = 5\n",
        "rows = num_parts // columns + 1\n",
        "fig=plt.figure(figsize=(columns*5, rows*5))\n",
        "for i in range(1, columns*rows + 1):\n",
        "  img = to_pil(image_parts[part_indices[i-1]])\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.margins(y=10)\n",
        "  plt.imshow(img)\n",
        "  plt.text(0, -5, str(i-1), fontsize=12)\n",
        "  plt.axis(\"off\")\n",
        "  if i >= num_parts:\n",
        "    break\n",
        "plt.show()\n",
        "\n",
        "ordered_part_indices = []\n",
        "for p in part_indices[:num_parts]:\n",
        "  size = part_sizes[p]\n",
        "  ordered_part_indices.append((size, p))\n",
        "\n",
        "ordered_part_indices.sort(reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uTvRf583fPX"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "num_initial_layouts = 100\n",
        "num_ctrl_ponts = 5\n",
        "init_rand_amount = 0.25\n",
        "num_shapes = 30\n",
        "\n",
        "resize_factor = 1.0\n",
        "new_img_size = int(img_size*resize_factor)\n",
        "\n",
        "bg_x = torch.linspace(0, img_size-1, num_ctrl_ponts).to(device)\n",
        "bgvals = (0.5 + init_rand_amount/2.0 * torch.rand(size=(3, num_ctrl_ponts))).to(device) \n",
        "bgvals.requires_grad = True\n",
        "bg_xs = torch.linspace(0, img_size-1, img_size).to(device)\n",
        "\n",
        "img_0 = interp(bg_x.cpu(), bgvals[0].cpu(), bg_xs.cpu()).to(device)\n",
        "img_1 = interp(bg_x.cpu(), bgvals[1].cpu(), bg_xs.cpu()).to(device)\n",
        "img_2 = interp(bg_x.cpu(), bgvals[2].cpu(), bg_xs.cpu()).to(device)\n",
        "img = torch.vstack([img_0, img_1, img_2])\n",
        "\n",
        "img = img.permute(1,0)\n",
        "img = img.tile((img_size, 1, 1))\n",
        "img = img.unsqueeze(0)\n",
        "img = img.permute(0, 3, 2, 1) # NHWC -> NCHW\n",
        "img = torch.nn.functional.interpolate(img, scale_factor=resize_factor, mode=\"bilinear\")\n",
        "\n",
        "bg_img = img.clone()\n",
        "\n",
        "image_list = []\n",
        "param_list = []\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "print(\"Creating\", num_initial_layouts, \"layouts for analysis\")\n",
        "\n",
        "for j in tqdm(range(num_initial_layouts)):\n",
        "  img = bg_img.clone()\n",
        "  partvals = torch.zeros(size=(num_shapes, 2)).to(device)\n",
        "\n",
        "  for i, index in enumerate(ordered_part_indices[:num_shapes]):\n",
        "    # get the part\n",
        "    part = parts_rgb[index[1]].to(device)\n",
        "    mask = parts_a[index[1]].to(device)\n",
        "\n",
        "    # scale\n",
        "    scale_factor = torch.tensor([new_img_size/2000.0, new_img_size/2000.0]).to(device)\n",
        "    part = kornia.geometry.transform.scale(part[None, :], scale_factor[None, :]).squeeze()\n",
        "    mask = kornia.geometry.transform.scale(mask[None, :], scale_factor[None, :]).squeeze()\n",
        "\n",
        "    # pad\n",
        "    h = part.shape[1]\n",
        "    w = part.shape[2]\n",
        "    lft_pad = (new_img_size - w)//2\n",
        "    top_pad = (new_img_size - h)//2\n",
        "    rgt_pad = new_img_size - w - lft_pad\n",
        "    bot_pad = new_img_size - h - top_pad\n",
        "    part = T.functional.pad(part, (lft_pad, top_pad, rgt_pad, bot_pad))\n",
        "    mask = T.functional.pad(mask, (lft_pad, top_pad, rgt_pad, bot_pad))\n",
        "\n",
        "    # translate\n",
        "    w_range = new_img_size - w * new_img_size/2000\n",
        "    h_range = new_img_size - h * new_img_size/2000\n",
        "    partvals[i][1] = (random.random()-0.5) * h_range / new_img_size\n",
        "    partvals[i][0] = (random.random()-0.5) * w_range / new_img_size\n",
        "    trans = partvals[i] * new_img_size\n",
        "    part = kornia.geometry.transform.translate(part[None, :], trans[None, :]).squeeze()\n",
        "    mask = kornia.geometry.transform.translate(mask[None, :], trans[None, :]).squeeze()\n",
        "\n",
        "    # composite the part\n",
        "    img *= 1-mask\n",
        "    img += part\n",
        "\n",
        "  image_list.append(img)\n",
        "  param_list.append(partvals.clone())\n",
        "\n",
        "layout_input = torch.stack(image_list).cuda().squeeze()\n",
        "with torch.no_grad():\n",
        "  layout_features = clip_model.encode_image(layout_input).float().cpu() \n",
        "layout_features /= layout_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "layout_similarity = query_features.cpu().numpy() @ layout_features.numpy().T\n",
        "layout_similarity = layout_similarity[0]\n",
        "layout_scores, layout_indices = get_top_N_semantic_similarity(layout_similarity, N=num_parts)\n",
        "\n",
        "img = image_list[layout_indices[0]]\n",
        "image = img.detach().cpu().numpy()\n",
        "image = np.transpose(image, (0, 2, 3, 1))[0]\n",
        "image = np.clip(image*255, 0, 255).astype(np.uint8)\n",
        "image_pil = Image.fromarray(image)\n",
        "plt.figure(figsize=(5, 5))\n",
        "img = plt.imshow(image_pil)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x056HMOtFyfc"
      },
      "outputs": [],
      "source": [
        "num_steps = 100\n",
        "color_lr = 0.005\n",
        "parts_lr = 0.01\n",
        "num_augmentations = 32\n",
        "\n",
        "text_features = get_text_features(prompt)\n",
        "\n",
        "augment_trans = T.Compose([\n",
        "  T.RandomPerspective(fill=1, p=1, distortion_scale=0.5),\n",
        "  T.RandomResizedCrop(img_size, scale=(0.7,0.9)),\n",
        "  T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "  T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "bg_x = torch.linspace(0, img_size-1, num_ctrl_ponts).to(device)\n",
        "bgvals = (0.5 + init_rand_amount/2.0 * torch.rand(size=(3, num_ctrl_ponts))).to(device) \n",
        "bgvals.requires_grad = True\n",
        "bg_xs = torch.linspace(0, img_size-1, img_size).to(device)\n",
        "# print(bgvals)\n",
        "\n",
        "partvals = param_list[layout_indices[0]].clone()\n",
        "partvals.requires_grad = True\n",
        "\n",
        "bg_optim = torch.optim.Adam([{'params': bgvals,   'lr': color_lr},\n",
        "                             {'params': partvals, 'lr': parts_lr}])\n",
        "loss_fn = torch.nn.CosineEmbeddingLoss()\n",
        "target = torch.full((1,32), fill_value=1.0).squeeze().to(device)\n",
        "\n",
        "# Run the main optimization loop\n",
        "for t in range(num_steps+1):\n",
        "  bg_optim.zero_grad()\n",
        "\n",
        "  img_0 = interp(bg_x.cpu(), bgvals[0].cpu(), bg_xs.cpu()).to(device)\n",
        "  img_1 = interp(bg_x.cpu(), bgvals[1].cpu(), bg_xs.cpu()).to(device)\n",
        "  img_2 = interp(bg_x.cpu(), bgvals[2].cpu(), bg_xs.cpu()).to(device)\n",
        "  img = torch.vstack([img_0, img_1, img_2])\n",
        "\n",
        "  img = img.permute(1,0)\n",
        "  img = img.tile((img_size, 1, 1))\n",
        "  img = img.unsqueeze(0)\n",
        "  img = img.permute(0, 3, 2, 1) # NHWC -> NCHW\n",
        "\n",
        "  for index, params in zip(ordered_part_indices[:num_shapes], partvals):\n",
        "    # get the part\n",
        "    part = parts_rgb[index[1]].to(device)\n",
        "    mask = parts_a[index[1]].to(device)\n",
        "\n",
        "    # scale\n",
        "    scale_factor = torch.tensor([img_size/2000.0, img_size/2000.0]).to(device)\n",
        "    part = kornia.geometry.transform.scale(part[None, :], scale_factor[None, :]).squeeze()\n",
        "    mask = kornia.geometry.transform.scale(mask[None, :], scale_factor[None, :]).squeeze()\n",
        "\n",
        "    # pad\n",
        "    h = part.shape[1]\n",
        "    w = part.shape[2]\n",
        "    lft_pad = (img_size - w)//2\n",
        "    top_pad = (img_size - h)//2\n",
        "    rgt_pad = img_size - w - lft_pad\n",
        "    bot_pad = img_size - h - top_pad\n",
        "    part = T.functional.pad(part, (lft_pad, top_pad, rgt_pad, bot_pad))\n",
        "    mask = T.functional.pad(mask, (lft_pad, top_pad, rgt_pad, bot_pad))\n",
        "\n",
        "    # translate\n",
        "    # trans = (params-0.5) * 150\n",
        "    trans = params * img_size\n",
        "    part = kornia.geometry.transform.translate(part[None, :], trans[None, :]).squeeze()\n",
        "    mask = kornia.geometry.transform.translate(mask[None, :], trans[None, :]).squeeze()\n",
        "\n",
        "    # composite the part\n",
        "    img *= 1-mask\n",
        "    img += part\n",
        "\n",
        "  img_augs = []\n",
        "  for n in range(num_augmentations):\n",
        "    img_augs.append(augment_trans(img))\n",
        "  im_batch = torch.cat(img_augs)\n",
        "  image_features = clip_model.encode_image(im_batch)\n",
        "  loss = loss_fn(image_features, text_features, target)\n",
        "\n",
        "  loss.backward()\n",
        "  bg_optim.step()\n",
        "  if t % 10 == 0:\n",
        "    print(\"-\" * 10)\n",
        "    image = img.detach().cpu().numpy()\n",
        "    image = np.transpose(image, (0, 2, 3, 1))[0]\n",
        "    image = np.clip(image*255, 0, 255).astype(np.uint8)\n",
        "    image_pil = Image.fromarray(image)\n",
        "    print('render loss:', loss.item())\n",
        "    print('iteration:', t)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    img = plt.imshow(image_pil)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYYyGEp4wzSG"
      },
      "outputs": [],
      "source": [
        "resize_factor = 4.0\n",
        "new_img_size = int(img_size*resize_factor)\n",
        "\n",
        "img_0 = interp(bg_x.cpu(), bgvals[0].cpu(), bg_xs.cpu()).to(device)\n",
        "img_1 = interp(bg_x.cpu(), bgvals[1].cpu(), bg_xs.cpu()).to(device)\n",
        "img_2 = interp(bg_x.cpu(), bgvals[2].cpu(), bg_xs.cpu()).to(device)\n",
        "img = torch.vstack([img_0, img_1, img_2])\n",
        "\n",
        "img = img.permute(1,0)\n",
        "img = img.tile((img_size, 1, 1))\n",
        "img = img.unsqueeze(0)\n",
        "img = img.permute(0, 3, 2, 1) # NHWC -> NCHW\n",
        "img = torch.nn.functional.interpolate(img, scale_factor=resize_factor, mode=\"bilinear\")\n",
        "\n",
        "for index, params in zip(ordered_part_indices, partvals):\n",
        "  # get the part\n",
        "  part = parts_rgb[index[1]].to(device)\n",
        "  mask = parts_a[index[1]].to(device)\n",
        "\n",
        "  # scale\n",
        "  scale_factor = torch.tensor([new_img_size/2000.0, new_img_size/2000.0]).to(device)\n",
        "  part = kornia.geometry.transform.scale(part[None, :], scale_factor[None, :]).squeeze()\n",
        "  mask = kornia.geometry.transform.scale(mask[None, :], scale_factor[None, :]).squeeze()\n",
        "\n",
        "  # pad\n",
        "  h = part.shape[1]\n",
        "  w = part.shape[2]\n",
        "  lft_pad = (new_img_size - w)//2\n",
        "  top_pad = (new_img_size - h)//2\n",
        "  rgt_pad = new_img_size - w - lft_pad\n",
        "  bot_pad = new_img_size - h - top_pad\n",
        "  part = T.functional.pad(part, (lft_pad, top_pad, rgt_pad, bot_pad))\n",
        "  mask = T.functional.pad(mask, (lft_pad, top_pad, rgt_pad, bot_pad))\n",
        "\n",
        "  # translate\n",
        "  # trans = (params-0.5) * 150 * resize_factor\n",
        "  trans = params * new_img_size\n",
        "  part = kornia.geometry.transform.translate(part[None, :], trans[None, :]).squeeze()\n",
        "  mask = kornia.geometry.transform.translate(mask[None, :], trans[None, :]).squeeze()\n",
        "\n",
        "  # composite the part\n",
        "  img *= 1-mask\n",
        "  img += part\n",
        "\n",
        "image = img.detach().cpu().numpy()\n",
        "image = np.transpose(image, (0, 2, 3, 1))[0]\n",
        "image = np.clip(image*255, 0, 255).astype(np.uint8)\n",
        "image_pil = Image.fromarray(image)\n",
        "image_pil.save(\"out.png\")\n",
        "\n",
        "import IPython\n",
        "IPython.display.Image(\"out.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CLIP and PASTE",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}